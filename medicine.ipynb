{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCAN842hgT7xuWMEK876ft",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/delibae/health_care_pr/blob/pr2%2Fbhj/medicine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Proprecessing"
      ],
      "metadata": {
        "id": "cAofyuZjka4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "c3_LXu3Suf6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create DataLoader"
      ],
      "metadata": {
        "id": "oxkghkajlY7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## define get_img(cv2)"
      ],
      "metadata": {
        "id": "VD3spDHAlrXb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WITlJzm8kQ-T"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "def get_img_list(series) :\n",
        "\n",
        "    \n",
        "    reshaped_image_list = []\n",
        "\n",
        "    for file_name in series :\n",
        "        image_path = file_name\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "\n",
        "        image = cv2.resize(image, dsize=(100, 100), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        reshaped_image = image.reshape(3,100,100) #reshape\n",
        "        reshaped_image_list.append(reshaped_image)\n",
        "        \n",
        "    return reshaped_image_list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# define get_transform(normalize)"
      ],
      "metadata": {
        "id": "W_XZOc2ylurC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transform():\n",
        "\n",
        "    t = list()\n",
        "    t.append(transforms.ToTensor())\n",
        "\n",
        "    t.append(transforms.Normalize(mean=(.5),std=(.5)))\n",
        "    \n",
        "    return transforms.Compose(t)"
      ],
      "metadata": {
        "id": "UCBOYaANlgRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# define dataset"
      ],
      "metadata": {
        "id": "AN8rNJUIlxUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, img_list, label=None):\n",
        "        self.transform = get_transform()\n",
        "        self.img_list = img_list\n",
        "        self.label = label\n",
        "        self.transformed_img_list = list(map(self.transform, self.img_list))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        x_data = self.transformed_img_list[i]\n",
        "\n",
        "        return x_data, torch.tensor(self.label[i],dtype = torch.long)"
      ],
      "metadata": {
        "id": "TCZgsxwMlioe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# define dataloader"
      ],
      "metadata": {
        "id": "MhP77XfElzJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_dataloader(x, y):\n",
        "    img_list = get_img_list(x)\n",
        "    dataset = CustomDataset(img_list=img_list, label=y)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size)\n",
        "    total_batch = math.ceil(len(dataset)/batch_size)\n",
        "\n",
        "    return dataloader, total_batch"
      ],
      "metadata": {
        "id": "u_KXOzR7ljxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create dataloader"
      ],
      "metadata": {
        "id": "wwQH7YfYtHSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "x_train = train['file_name']\n",
        "x_valid = valid['file_name']\n",
        "y_train = train['label']\n",
        "y_valid = valid['label']\n",
        "\n",
        "train_dataloader, train_total_batch = get_dataloader(x_train, y_train)\n",
        "valid_dataloader, valid_total_batch = get_dataloader(x_valid, y_valid)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qQ5LRxqMtJKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "5-jVQONBkgIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "M6k77r-NklYZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model structure"
      ],
      "metadata": {
        "id": "iefgA3olkvS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class CNN(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(CNN, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=20, kernel_size=5, stride=1)\n",
        "#         self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5, stride=1)\n",
        "#         self.fc1 = nn.Linear(22 * 22 * 50, 500)\n",
        "#         self.fc2 = nn.Linear(500, 6)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.conv1(x))\n",
        "#         x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "#         x = F.relu(self.conv2(x))\n",
        "#         x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "#         x = x.view(-1, 50*22*22)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = self.fc2(x)\n",
        "#         return x"
      ],
      "metadata": {
        "id": "DPxrXWpQki9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "\n",
        "            nn.Conv2d(3, 64, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(512, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        #512 4 4\n",
        "        self.avg_pool = nn.AvgPool2d(4)\n",
        "        #512 1 1\n",
        "        self.classifier = nn.Linear(512, 1000)\n",
        "        \"\"\"\n",
        "        self.fc1 = nn.Linear(512*2*2,4096)\n",
        "        self.fc2 = nn.Linear(4096,4096)\n",
        "        self.fc3 = nn.Linear(4096,10)\n",
        "        \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(x.shape)\n",
        "        features = self.conv(x)\n",
        "        print(x.shape)\n",
        "        x = self.avg_pool(features)\n",
        "        print(x.shape)\n",
        "        #print(avg_pool.size())\n",
        "        x = x.view(features.size(0), -1)\n",
        "        print(x.shape)\n",
        "        #print(flatten.size())\n",
        "        x = self.classifier(x)\n",
        "        print(x.shape)\n",
        "        #x = self.softmax(x)\n",
        "        return x, features\n",
        "\n"
      ],
      "metadata": {
        "id": "ZivOSIUVoEY2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## device"
      ],
      "metadata": {
        "id": "MOxLwbd4tmdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XglXFt1itnfO",
        "outputId": "e3152aaa-2b2e-42df-9ede-133b154dd14b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create model(loss function / optimizer)"
      ],
      "metadata": {
        "id": "KGYcG5OVtdJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = VGG().to(device)\n",
        "\n",
        "model.train()\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "qZP1qdqSo5WR"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test model"
      ],
      "metadata": {
        "id": "6b9f0E9rt1D4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp = torch.randn(2,3,128,128)\n",
        "model(inp)"
      ],
      "metadata": {
        "id": "Q1KERoX4o71Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "JcyVGHAtkyN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## start train"
      ],
      "metadata": {
        "id": "l9r4m5UPlCkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_l = []\n",
        "validloss_l = []\n",
        "epoch_num = 30\n",
        "\n",
        "for epoch in range(epoch_num):\n",
        "  for index, (data, target) in enumerate(train_dataloader):\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "    optimizer.zero_grad()  # 기울기 초기화\n",
        "    output = model(data)\n",
        "    loss = criterion(output, target)\n",
        "    loss.backward()  # 역전파\n",
        "    optimizer.step()\n",
        "\n",
        "  for   index, (data, target) in enumerate(valid_dataloader):\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "    output = model(data)\n",
        "    valid_loss = criterion(output,target)\n",
        "\n",
        "  print(f\"loss of {epoch} epoch, {loss.item():.3f} / valid loss: {valid_loss:.3f}\")\n",
        "  loss_l.append(loss.item())\n",
        "  validloss_l.append(valid_loss.item())\n"
      ],
      "metadata": {
        "id": "9HJEnNImkxa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## plot loss"
      ],
      "metadata": {
        "id": "R25u9FWXk_6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "RB5VGIHQuVC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_l, label = \"train loss\")\n",
        "plt.plot(validloss_l, label = \"valid loss\")"
      ],
      "metadata": {
        "id": "pUiT5sJJlBOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Analysis"
      ],
      "metadata": {
        "id": "Rsg7RJXwlFIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix"
      ],
      "metadata": {
        "id": "nhD4e_HvlISO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "cEhaYi5vlHzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "# iterate over test data\n",
        "for index, (inputs, labels) in enumerate(valid_dataloader):\n",
        "        output = model(inputs) # Feed Network\n",
        "\n",
        "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
        "        y_pred.extend(output) # Save Prediction\n",
        "        \n",
        "        labels = labels.data.cpu().numpy()\n",
        "        y_true.extend(labels) # Save Truth"
      ],
      "metadata": {
        "id": "Ah5ePDBTlMlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = label_list"
      ],
      "metadata": {
        "id": "DAqlZXXHlUQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "TM3RQs2XlU01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cf_matrix = confusion_matrix(y_true, y_pred, normalize = 'pred')\n",
        "df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
        "                     columns = [i for i in classes])\n",
        "plt.figure(figsize = (12,7))\n",
        "sns.heatmap(df_cm, annot=True)\n",
        "plt.show"
      ],
      "metadata": {
        "id": "_ihwPCtUlVkc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}